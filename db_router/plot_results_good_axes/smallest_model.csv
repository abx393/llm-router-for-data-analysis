task_name,spec_task_name,workload_file,neighbor_1,neighbor_2,neighbor_3,chosen_model,router_reason,eval_accuracy,eval_avg_latency,error
,task195_sentiment140_classification,airline_binary_sentiment.csv,task195_sentiment140_classification,task819_pec_sentiment_classification,task833_poem_sentiment_classification,vader,TEST OVERRIDE: max_latency=0 → forcing 'vader' (ignoring candidate results and constraints).,0.6590909090909091,0.0001282746141607111,
,task1312_amazonreview_polarity_classification,amazon_polarity_binary_sentiment.csv,task1312_amazonreview_polarity_classification,task833_poem_sentiment_classification,task363_sst2_polarity_classification,vader,TEST OVERRIDE: max_latency=0 → forcing 'vader' (ignoring candidate results and constraints).,0.71,0.0004872965812683105,
,task888_reviews_classification,imdb_binary_sentiment.csv,task888_reviews_classification,task477_cls_english_dvd_classification,task493_review_polarity_classification,vader,TEST OVERRIDE: max_latency=0 → forcing 'vader' (ignoring candidate results and constraints).,0.6,0.0026850295066833496,
,task888_reviews_classification,rotten_tomatoes_binary_sentiment.csv,task888_reviews_classification,task477_cls_english_dvd_classification,task363_sst2_polarity_classification,vader,TEST OVERRIDE: max_latency=0 → forcing 'vader' (ignoring candidate results and constraints).,0.68,7.70115852355957e-05,
,task195_sentiment140_classification,tweet_eval_sentiment_binary_sentiment.csv,task195_sentiment140_classification,task819_pec_sentiment_classification,task833_poem_sentiment_classification,vader,TEST OVERRIDE: max_latency=0 → forcing 'vader' (ignoring candidate results and constraints).,0.69,6.46495819091797e-05,
,task475_yelp_polarity_classification,yelp_polarity_binary_sentiment.csv,task475_yelp_polarity_classification,task746_yelp_restaurant_review_classification,task819_pec_sentiment_classification,vader,TEST OVERRIDE: max_latency=0 → forcing 'vader' (ignoring candidate results and constraints).,0.72,0.00119002103805542,
